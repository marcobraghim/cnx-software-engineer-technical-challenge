## **Problema:**

Esse cÃ³digo **pode derrubar seu backend** se muitas pessoas fizerem upload simultÃ¢neo porque:

1. âŒ **MemÃ³ria explode** - Cada CSV enorme carrega na RAM
2. âŒ **CPU saturada** - MÃºltiplos `parseCSV()` rodando juntos
3. âŒ **DB sobrecarregado** - MÃºltiplos bulk inserts simultÃ¢neos
4. âŒ **Sem limite de concorrÃªncia** - Aceita uploads ilimitados

---

## **SoluÃ§Ãµes:**

### **1. Offload para Cloud Storage + Cloud Function (Recomendada) â­â­â­**

```typescript
// Backend - sÃ³ faz upload pro Storage
async function uploadCSV(file) {
  // Upload pro Cloud Storage (rÃ¡pido, nÃ£o processa)
  const fileName = `uploads/${Date.now()}-${file.originalname}`;
  await storage.bucket('my-bucket').upload(file.path, {
    destination: fileName,
  });
  
  // Salva registro no DB
  const job = await db.jobs.create({
    fileName,
    status: 'pending',
    uploadedAt: new Date(),
  });
  
  // Retorna imediatamente
  return { message: 'Arquivo recebido!', jobId: job.id };
}

// Cloud Function - processa quando tem recurso
export async function processCSV(event) {
  const file = event.data; // Trigger automÃ¡tico
  
  // Streaming (nÃ£o carrega tudo na memÃ³ria)
  const stream = storage.bucket('my-bucket').file(file.name).createReadStream();
  
  let batch = [];
  
  stream
    .pipe(csv())
    .on('data', async (row) => {
      batch.push(row);
      
      if (batch.length >= 1000) {
        await db.batchInsert(batch);
        batch = [];
      }
    })
    .on('end', async () => {
      if (batch.length > 0) {
        await db.batchInsert(batch);
      }
      await db.jobs.update(file.jobId, { status: 'completed' });
    });
}
```

**Por quÃª Ã© melhor:**
- âœ… Backend sÃ³ faz upload (segundos)
- âœ… Cloud Function processa (escala automaticamente)
- âœ… Streaming (nÃ£o estoura memÃ³ria)
- âœ… Cada upload tem recursos isolados

---

### **2. Fila de processamento com concorrÃªncia limitada â­â­**

```typescript
import pLimit from 'p-limit';
import { Queue } from 'bull'; // ou BullMQ

const uploadQueue = new Queue('csv-uploads', {
  redis: { host: 'localhost', port: 6379 },
  limiter: {
    max: 3, // Apenas 3 uploads processando por vez
    duration: 1000,
  },
});

// Backend
async function uploadCSV(file) {
  // Adiciona na fila
  const job = await uploadQueue.add({
    filePath: file.path,
    userId: req.user.id,
  });
  
  return { message: 'Na fila de processamento', jobId: job.id };
}

// Worker (pode rodar em outro servidor)
uploadQueue.process(3, async (job) => {
  const { filePath } = job.data;
  
  // Streaming
  const emails = [];
  await parseCSVStream(filePath, async (batch) => {
    await db.batchInsert(batch);
    job.progress(batch.length); // Atualiza progresso
  });
  
  return { processed: emails.length };
});
```

**Por quÃª Ã© melhor:**
- âœ… Limita processamento simultÃ¢neo (3 por vez)
- âœ… Backend nÃ£o processa (sÃ³ enfileira)
- âœ… Worker pode estar em outro servidor
- âœ… UsuÃ¡rio pode consultar progresso

---

### **3. Pub/Sub + mÃºltiplas Cloud Functions â­â­**

```typescript
// Backend
async function uploadCSV(file) {
  const fileName = await uploadToStorage(file);
  
  // Publica evento
  await pubsub.topic('csv-uploaded').publish(
    Buffer.from(JSON.stringify({ fileName }))
  );
  
  return { message: 'Processando...' };
}

// Cloud Function (escala automaticamente)
export async function processCSVPubSub(message) {
  const { fileName } = JSON.parse(
    Buffer.from(message.data, 'base64').toString()
  );
  
  // Processa com streaming
  await processFileWithStreaming(fileName);
}
```

---

### **4. Melhorar cÃ³digo atual (mÃ­nimo) â­**

```typescript
import pLimit from 'p-limit';

// Limitar uploads simultÃ¢neos globalmente
const uploadLimit = pLimit(5); // MÃ¡ximo 5 processando

async function uploadCSV(file) {
  return uploadLimit(async () => {
    response.send({ message: 'Processando...' });
    
    // Streaming (nÃ£o carrega tudo)
    const emails = [];
    await parseCSVStream(file, async (batch) => {
      await db.batchInsert(batch);
    });
  });
}

// Parse com streaming
async function parseCSVStream(file, onBatch) {
  return new Promise((resolve, reject) => {
    let batch = [];
    
    fs.createReadStream(file.path)
      .pipe(csv())
      .on('data', async (row) => {
        batch.push(row);
        
        if (batch.length >= 1000) {
          await onBatch(batch);
          batch = [];
        }
      })
      .on('end', async () => {
        if (batch.length > 0) await onBatch(batch);
        resolve();
      })
      .on('error', reject);
  });
}
```

---

## **ComparaÃ§Ã£o:**

| Abordagem | Escala | Complexidade | Custo | Backend trava? |
|-----------|--------|--------------|-------|----------------|
| **Cloud Storage + Function** | â­â­â­ | â­â­ | â­â­ | âŒ NÃ£o |
| **Fila (Bull/Redis)** | â­â­ | â­â­â­ | â­â­ | âŒ NÃ£o |
| **Pub/Sub** | â­â­â­ | â­â­ | â­â­ | âŒ NÃ£o |
| **Limite + Streaming** | â­ | â­ | â­â­â­ | âš ï¸ Pode |

---

## **RecomendaÃ§Ã£o final:**

**Use Cloud Storage + Cloud Function:**

```
1. User upload â†’ Backend â†’ Cloud Storage (rÃ¡pido)
2. Storage trigger â†’ Cloud Function (automÃ¡tico)
3. Cloud Function processa streaming
4. Insere no DB + cria Cloud Tasks
```

**Arquitetura completa:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Upload  â”‚ â†’ Backend (sÃ³ salva no Storage)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â†“
              Cloud Storage
                    â†“ (trigger)
              Cloud Function
              (processa CSV)
                    â†“
              Database (bulk insert)
                    â†“
              Cloud Tasks
              (1 task por email)
                    â†“
              Cloud Function
              (envia email 1 a cada 10s)
```

**Vantagens:**
- âœ… Backend **nunca** trava
- âœ… Escala automaticamente
- âœ… Cada upload isolado
- âœ… Pode processar milhÃµes de CSVs simultÃ¢neos

**Custo estimado:**
- 1 milhÃ£o de emails processados: ~$5-10 USD

Quer um exemplo de cÃ³digo completo dessa arquitetura? ğŸš€